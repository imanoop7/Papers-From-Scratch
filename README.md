# AI Papers Implementation from Scratch

This repository contains implementations of fundamental AI/ML papers from scratch. Each implementation focuses on understanding the core concepts by building models using minimal dependencies. New implementations are added weekly.

## Overview

Each implementation includes:
- Pure Python/NumPy version for fundamental understanding
- PyTorch version for practical usage
- Detailed documentation and explanations
- Training scripts and examples
- Visualization tools where applicable

## Implementations

### 1. Attention Is All You Need (Transformer)
Implementation of the original Transformer model as described in the [Vaswani et al. 2017 paper](https://arxiv.org/abs/1706.03762).
- Multi-head attention mechanism
- Positional encoding
- Encoder-decoder architecture
- Scaled dot-product attention

### 2. LSTM Networks
Implementation of Long Short-Term Memory networks based on the [Hochreiter & Schmidhuber 1997 paper](https://www.bioinf.jku.at/publications/older/2604.pdf).
- LSTM cell with forget gate
- Input, output, and cell state
- Backpropagation through time
- Sequence prediction examples

### 3. ResNet Architecture
Implementation of Residual Networks from the [He et al. 2015 paper](https://arxiv.org/abs/1512.03385).
- Skip connections
- Residual blocks
- Batch normalization
- Deep architecture variants (ResNet-18, 34, 50)

### 4. Recurrent Neural Networks (RNN)
Basic RNN implementation inspired by classic architectures.
- Simple RNN cells
- Sequence modeling
- Character-level language model
- Time series prediction examples

### 5. Generative Adversarial Networks (GANs)
Implementation based on the [Goodfellow et al. 2014 paper](https://arxiv.org/abs/1406.2661).
- Generator and discriminator networks
- Adversarial training
- MNIST generation example
- Training visualization

### 6. U-Net Architecture
Implementation from the [Ronneberger et al. 2015 paper](https://arxiv.org/abs/1505.04597).
- Encoder-decoder with skip connections
- Image segmentation
- Biomedical image processing
- Up/down-sampling operations

### 7. Vision Transformer (ViT)
Implementation based on the [Dosovitskiy et al. 2020 paper](https://arxiv.org/abs/2010.11929).
- Patch embedding
- Position embeddings
- Multi-head attention for images
- Image classification examples

### 8. Word2Vec
Implementation of the CBOW model from the [Mikolov et al. 2013 paper](https://arxiv.org/abs/1301.3781).
- Continuous Bag of Words (CBOW)
- Negative sampling
- Word embeddings
- Text analysis examples

### 9. GPT Model
Basic implementation inspired by the [GPT architecture](https://arxiv.org/abs/2005.14165).
- Transformer decoder
- Autoregressive language modeling
- Text generation
- Pre-training and fine-tuning examples

### 10. AlexNet
Implementation of the [Krizhevsky et al. 2012 paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).
- Convolutional neural network
- ReLU activations
- Dropout regularization
- ImageNet classification

### 11. Autoencoders
Implementation of various autoencoder architectures.
- Basic autoencoder
- Denoising autoencoder
- Variational autoencoder (VAE)
- Dimensionality reduction examples

